{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 教學目標:\n",
    "\n",
    "了解 Convolution 卷積的組成\n",
    "\n",
    "\n",
    "\n",
    "## 範例內容:\n",
    "\n",
    "1.定義單步的卷積\n",
    "\n",
    "2.輸出卷積的計算值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_single_step\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    定義一層 Kernel (內核), 使用的參數說明如下\n",
    "    Arguments:\n",
    "        a_slice_prev -- 輸入資料的維度\n",
    "        W -- 權重, 被使用在 a_slice_prev\n",
    "        b -- 偏差參數 \n",
    "    Returns:\n",
    "        Z -- 滑動窗口（W，b）卷積在前一個 feature map 上的結果\n",
    "    \"\"\"\n",
    "\n",
    "    # 定義一個元素介於 a_slice and W\n",
    "    s = a_slice_prev * W\n",
    "    # 加總所有的 \"s\" 並指定給Z.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. 這是 float() 函數, \n",
    "    Z = float(Z + b)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "seed( ) 用於指定隨機數生成時所用算法開始的整數值，\n",
    "如果使用相同的seed( )值，則每次生成的隨即數都相同，\n",
    "如果不設置這個值，則係統根據時間來自己選擇這個值，\n",
    "此時每次生成的隨機數因時間差異而不同。\n",
    "'''\n",
    "np.random.seed(1)\n",
    "#定義一個 4x4x3 的 feature map\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "#取得計算後,卷績矩陣的值\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標:\n",
    "\n",
    "了解 Convolution 卷積的組成\n",
    "\n",
    "\n",
    "## 作業重點:\n",
    "\n",
    "修改 a_slice_prev, 檢查 Z 的輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# GRADED FUNCTION: conv_single_step\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    定義一層 Kernel (內核), 使用的參數說明如下\n",
    "    Arguments:\n",
    "        a_slice_prev -- 輸入資料的維度\n",
    "        W -- 權重, 被使用在 a_slice_prev\n",
    "        b -- 偏差參數 \n",
    "    Returns:\n",
    "        Z -- 滑動窗口（W，b）卷積在前一個 feature map 上的結果\n",
    "    \"\"\"\n",
    "\n",
    "    # 定義一個元素介於 a_slice and W\n",
    "    s = a_slice_prev * W\n",
    "    # 加總所有的 \"s\" 並指定給Z.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. 這是 float() 函數,\n",
    "    Z = float(Z + b)\n",
    "\n",
    "    return Z\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Your Code Here :\n",
    "--------------------\n",
    "\n",
    "np.random.seed(1)\n",
    "#定義一個 a * a * d 的 feature map\n",
    "a_slice_prev = \n",
    "W = \n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "#取得計算後,卷積矩陣的值\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)\n",
    "'''\n",
    "\n",
    "np.random.seed(1)\n",
    "#定義一個 4 x 4 x 3 的 feature map\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "#取得計算後,卷績矩陣的值\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 看 ANS 後 改一版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = 11.954675594834528\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "#定義一個 a*a*d 的 feature map\n",
    "a_slice_prev = np.random.randn(16, 16, 1)\n",
    "W = np.random.randn(16, 16, 1)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "#取得計算後,卷積矩陣的值\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = 30.046217995633093\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "#定義一個 a*a*d 的 feature map\n",
    "a_slice_prev = np.random.randn(32, 32, 3)\n",
    "W = np.random.randn(32, 32, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "#取得計算後,卷積矩陣的值\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = 3.027931858262864\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "#定義一個 a*a*d 的 feature map\n",
    "a_slice_prev = np.random.randn(2, 2, 1)\n",
    "W = np.random.randn(2, 2, 1)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "#取得計算後,卷積矩陣的值\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考資料\n",
    "\n",
    "![img1](https://ai100-fileentity.cupoy.com/2nd/homework/D94/1564381721875/large)\n",
    "\n",
    "\n",
    "## 介紹三種視覺化方法： \n",
    "\n",
    "\n",
    "> ## 1. 卷積核輸出的視覺化(Visualizing intermediate convnet outputs (intermediate activations)，即視覺化卷積核經過啟動之後的結果。能夠看到圖像經過卷積之後結果，幫助理解卷積核的作用\n",
    ">![img1-1](http://static.zybuluo.com/jiemojiemo/bvjcdaf78o626pxx8c1mh7ip/2.png)\n",
    "\n",
    "\n",
    "> ## 2. 卷積核的視覺化(Visualizing convnets filters)，説明我們理解卷積核是如何感受圖像的\n",
    ">![img1-2](http://static.zybuluo.com/jiemojiemo/ek81shtj99ek8r5bqow3ro0r/33.png)\n",
    "\n",
    "\n",
    "\n",
    "> ## 3. 熱度圖視覺化(Visualizing heatmaps of class activation in an image)，通過熱度圖，瞭解圖像分類問題中圖像哪些部分起到了關鍵作用，同時可以定位圖像中物體的位置。\n",
    ">![img1-3](http://static.zybuluo.com/jiemojiemo/vp9t56d23lr0fnwz8lxmpfrl/66.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 卷積核輸出的視覺化(Visualizing intermediate convnet outputs (intermediate activations)\n",
    "\n",
    "\n",
    "想法很簡單：向CNN輸入一張圖像，獲得某些卷積層的輸出，視覺化該輸出。\n",
    "\n",
    "\n",
    "圖片來源：[Deep Learning with Python](https://blog.csdn.net/weiwei9363/article/details/79112872)\n",
    "\n",
    "\n",
    "## [處理影像的利器 - 卷積神經網路(Convolutional Neural Network)](https://dotblogs.com.tw/greengem/2017/12/17/094150)\n",
    "\n",
    "> ### 卷積層處理方式與影像處理方法類似，<font color=\"red\">採用滑動視窗(Sliding Window)運算，藉由給予『卷積核』不同的權重組合，就可以偵測形狀的邊、角，也有去除噪音(Noise)及銳化(Sharpen)的效果，萃取這些特徵當作辨識的依據，這也克服了迴歸(Regression)會受『異常點』(Outliers)嚴重影響推測結果的缺點</font>，好比說一個人的鼻子長了一顆痣，我們也應該能依據形狀辨識出那是鼻子。\n",
    "\n",
    "\n",
    "![img2](https://ai100-fileentity.cupoy.com/2nd/homework/D94/1564381933176/large)\n",
    "\n",
    "\n",
    "![img3](https://ai100-fileentity.cupoy.com/2nd/homework/D94/1564381942733/large)\n",
    "\n",
    "\n",
    "\n",
    "透過多層卷積/池化，萃取特徵當作 Input，再接至一到多個完全連接層，進行分類，這就是CNN的典型作法，下一篇我們就用 CNN 來作阿拉伯數字的辨識，看看有甚麼不同，緊接著，我們再介紹兩個 CNN 應用，說明 Neural Network 不是只能作分類而已。\n",
    "\n",
    "\n",
    "\n",
    "參考來源：https://dotblogs.com.tw/greengem/2017/12/17/094150\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "\n",
    "## 延伸閱讀\n",
    "\n",
    "## [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n",
    "\n",
    "![img4](https://ai100-fileentity.cupoy.com/2nd/homework/D94/1564382096604/large)\n",
    "\n",
    "\n",
    "![img5](https://ai100-fileentity.cupoy.com/2nd/homework/D94/1564382293987/large)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> ## 圖像是像素值的矩陣\n",
    "\n",
    "> 實質上，每個圖像都可以表示為像素值矩陣。\n",
    "> ![img6](https://ujwlkarn.files.wordpress.com/2016/08/8-gif.gif?w=192&h=192&zoom=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Convolution Step\n",
    "\n",
    "    ConvNets derive their name from the “convolution” operator. The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. We will not go into the mathematical details of Convolution here, but will try to understand how it works over images.\n",
    "\n",
    "\n",
    "    ConvNets從“卷積”運算符中獲取它們的名稱  。ConvNet的Convolution的主要目的是從輸入圖像中提取特徵。卷積通過使用小方塊輸入數據學習圖像特徵來保持像素之間的空間關係。我們不會在這裡討論卷積的數學細節，但會嘗試理解它對圖像的作用。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Channel is a conventional term used to refer to a certain component of an image. An image from a standard digital camera will have three channels – red, green and blue – you can imagine those as three 2d-matrices stacked over each other (one for each color), each having pixel values in the range 0 to 255.\n",
    "\n",
    "    A grayscale image, on the other hand, has just one channel. For the purpose of this post, we will only consider grayscale images, so we will have a single 2d matrix representing an image. The value of each pixel in the matrix will range from 0 to 255 – zero indicating black and 255 indicating white.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 理解Convolution操作的另一個好方法是查看 下面圖6中的動畫：\n",
    "\n",
    "![img7](https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif?w=480&zoom=2)\n",
    "\n",
    "    濾鏡（具有紅色輪廓）在輸入圖像（卷積操作）上滑動以產生特徵圖。另一個濾鏡（帶有綠色輪廓）在同一圖像上的捲積給出了不同的特徵圖，如圖所示。重要的是要注意Convolution操作捕獲原始圖像中的本地依賴性。還要注意這兩個不同的濾鏡如何從同一原始圖像生成不同的特徵圖。請記住，上面的圖像和兩個過濾器只是數字矩陣，如上所述。\n",
    "\n",
    "-\n",
    "\n",
    "## <font color=\"red\">特徵映射（卷積特徵）的大小由三個參數控制</font>，我們需要在執行卷積步驟之前確定它們：\n",
    "\n",
    "### <font color=\"red\">1. 深度：  </font>\n",
    "\n",
    "        Depth: \n",
    "\n",
    "        Depth corresponds to the number of filters we use for the convolution operation. In the network shown in Figure 7, we are performing convolution of the original boat image using three distinct filters, thus producing three different feature maps as shown. You can think of these three feature maps as stacked 2d matrices, so, the ‘depth’ of the feature map would be three.\n",
    "\n",
    "        深度對應於我們用於卷積運算的濾波器數量。在圖7所示的網絡中  ，我們使用三個不同的濾波器對原始船圖像進行卷積，從而產生三個不同的特徵圖，如圖所示。您可以將這三個要素圖視為堆疊的2d矩陣，因此，要素圖的“深度”將為3。\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "### <font color=\"red\">2. Stride： </font>\n",
    "\n",
    "        Stride: \n",
    "\n",
    "        Stride is the number of pixels by which we slide our filter matrix over the input matrix. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2, then the filters jump 2 pixels at a time as we slide them around. Having a larger stride will produce smaller feature maps.\n",
    "\n",
    "        Stride是 我們在輸入矩陣上滑動濾波器矩陣的像素數。當步幅為1時，我們一次移動濾波器一個像素。當步幅為2時，當我們滑動它們時，濾波器一次跳躍2個像素。更大的步幅將產生更小的特徵映射。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <font color=\"red\">3. 零填充：</font>\n",
    "\n",
    "        Zero-padding: \n",
    "\n",
    "        Sometimes, it is convenient to pad the input matrix with zeros around the border, so that we can apply the filter to bordering elements of our input image matrix. A nice feature of zero padding is that it allows us to control the size of the feature maps. Adding zero-padding is also called wide convolution, and not using zero-padding would be a narrow convolution. This has been explained clearly in [14].\n",
    "\n",
    "        有時，在邊界周圍用零填充輸入矩陣很方便，這樣我們就可以將濾波器應用於輸入圖像矩陣的邊界元素。零填充的一個很好的特性是它允許我們控制特徵映射的大小。添加零填充也稱為寬卷積，不使用零填充將是一個窄卷積。[ 14 ] 已經清楚地解釋了這一點。\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Introducing Non Linearity (ReLU) ：引入非線性（ReLU）\n",
    "\n",
    "在上面的圖3中的每個卷積操作之後，已經使用了稱為ReLU的附加操作。<font color=\"red\">ReLU代表整流線性單元，是一種非線性操作。</font>\n",
    "\n",
    "\n",
    "    ReLU是元素操作（按像素應用）並將要素圖中的所有負像素值替換為零。ReLU的目的是在我們的ConvNet中引入非線性，因為我們希望我們的ConvNet學習的大多數真實數據都是非線性的（卷積是線性操作 - 元素的矩陣乘法和加法，所以我們通過引入像ReLU這樣的非線性函數來解釋非線性。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Pooling Step 匯集步驟\n",
    "\n",
    "空間池（也稱為子採樣或下採樣）可降低每個要素圖的維度，但保留最重要的信息。空間池可以是不同類型：最大值，平均值，總和等。\n",
    "\n",
    "在Max Pooling的情況下，我們定義空間鄰域（例如，2×2窗口）並從該窗口內的整流特徵映射中獲取最大元素。我們也可以取平均值（平均合併數）或該窗口中所有元素的總和，而不是取最大元素。在實踐中，Max Pooling已被證明可以更好地工作。\n",
    "\n",
    "\n",
    "    Pooling的功能是逐步減小輸入表示的空間大小[ 4 ]。特別是匯集\n",
    "\n",
    "    - 使輸入表示（特徵維度）更小，更易於管理\n",
    "    \n",
    "    - 減少網絡中的參數和計算次數，因此，控製過度擬合  [ 4 ]\n",
    "    \n",
    "    - 使網絡對輸入圖像中的小變換，失真和平移不變（輸入中的小失真不會改變Pooling的輸出 - 因為我們採用局部鄰域中的最大/平均值）。\n",
    "    \n",
    "    - 幫助我們達到我們圖像的幾乎尺度不變的表示（確切的術語是“等變的”）。這是非常強大的，因為我們可以檢測圖像中的對象，無論它們位於何處（詳見[ 18 ]和[ 19 ]）。\n",
    "    \n",
    "    \n",
    "    \n",
    "    The function of Pooling is to progressively reduce the spatial size of the input representation [4]. In particular, pooling\n",
    "\n",
    "    - makes the input representations (feature dimension) smaller and more manageable\n",
    "    \n",
    "    - reduces the number of parameters and computations in the network, therefore, controlling overfitting [4]\n",
    "    \n",
    "    - makes the network invariant to small transformations, distortions and translations in the input image (a small distortion in input will not change the output of Pooling – since we take the maximum / average value in a local neighborhood).\n",
    "    \n",
    "    - helps us arrive at an almost scale invariant representation of our image (the exact term is “equivariant”). This is very powerful since we can detect objects in an image no matter where they are located (read [18] and [19] for details).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 完全連接層\n",
    "\n",
    "完全連接層是傳統的多層感知器，在輸出層使用softmax激活功能（也可以使用其他分類器，如SVM，但在本文中將堅持使用softmax）。術語“完全連接”意味著前一層中的每個神經元都連接到下一層的每個神經元。 如果您不熟悉多層感知器，我建議您閱讀這篇文章。\n",
    "\n",
    "卷積和池化層的輸出表示輸入圖像的高級特徵。完全連接層的目的是使用這些功能根據訓練數據集將輸入圖像分類為各種類。例如，我們要執行的圖像分類任務有四種可能的輸出，如下面的圖14所示（注意圖14沒有顯示完全連接層中節點之間的連接）\n",
    "\n",
    "> ![img8](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-06-at-12-34-02-am.png?w=968&h=304)\n",
    "> 圖14：完全連接的層 - 每個節點連接到相鄰層中的每個其他節點\n",
    "\n",
    "除了分類之外，添加完全連接的層也是學習這些特徵的非線性組合的（通常）廉價方式。卷積和池化層的大多數特徵可能對分類任務有利，但這些特徵的組合可能更好[ 11 ]。\n",
    "\n",
    "來自完全連接層的輸出概率之和為1.這通過使用[Softmax](http://cs231n.github.io/linear-classify/#softmax)作為完全連接層的輸出層中的激活函數來確保。Softmax函數採用任意實值得分的向量，並將其壓縮為0到1之間的值的向量，其總和為1。\n",
    "\n",
    "\n",
    "\n",
    "## 全部放在一起 - 使用反向傳播進行培訓\n",
    "\n",
    "## <font color=\"red\">如上所述，Convolution + Pooling層充當來自輸入圖像的特徵提取器，而完全連接層充當分類器。</font>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "\n",
    "## 卷積網絡的整體培訓過程可概括如下：\n",
    "\n",
    "步驟1：我們用隨機值初始化所有過濾器和參數/權重\n",
    "\n",
    "\n",
    "\n",
    "步驟2： 網絡將訓練圖像作為輸入，經過前向傳播步驟（卷積，ReLU和池化操作以及完全連接層中的前向傳播）並找到每個類的輸出概率。\n",
    "可以說上面船形圖的輸出概率是[0.2,0.4,0.1,0.3]\n",
    "由於權重是為第一個訓練示例隨機分配的，因此輸出概率也是隨機的。\n",
    "\n",
    "\n",
    "\n",
    "第3步：計算輸出層的總誤差（所有4個類的總和）\n",
    " $$ 總誤差=Σ½（目標概率 - 輸出概率）² $$\n",
    "\n",
    "\n",
    "\n",
    "步驟4：使用反向傳播計算相對於網絡中所有權重的誤差梯度，並使用梯度下降來更新所有濾波器值/權重和參數值，以最小化輸出誤差。\n",
    "權重的調整與它們對總誤差的貢獻成比例。\n",
    "當再次輸入相同的圖像時，輸出概率現在可能是[0.1,0.1,0.7,0.1]，這更接近目標矢量[0,0,1,0]。\n",
    "這意味著網絡已經學會通過調整其權重/濾波器來正確地對該特定圖像進行分類，從而減少輸出誤差。\n",
    "過濾器數量，過濾器大小，網絡體系結構等參數在步驟1之前都已修復，並且在訓練過程中不會更改 - 僅更新過濾器矩陣和連接權重的值。\n",
    "\n",
    "步驟5：對訓練集中的所有圖像重複步驟2-4。\n",
    "上述步驟  訓練  ConvNet - 這實際上意味著ConvNet的所有權重和參數現在已經過優化，可以正確分類來自訓練集的圖像。\n",
    "\n",
    "\n",
    "\n",
    "當一個新的（看不見的）圖像被輸入到ConvNet中時，網絡將經歷前向傳播步驟並輸出每個類的概率（對於新圖像，輸出概率使用已經優化的權重來計算以正確分類所有以前的訓練樣例）。如果我們的訓練集足夠大，網絡（希望）可以很好地推廣到新圖像並將它們分類為正確的類別。\n",
    "\n",
    "\n",
    "\n",
    "注1：上述步驟過於簡單，避免了數學細節，以便為培訓過程提供直觀的信息。有關數學公式和透徹理解，請參見[ 4 ]和[ 12 ]。\n",
    "\n",
    "\n",
    "\n",
    "注2：在上面的例子中，我們使用了兩組交替的Convolution和Pooling層。但請注意，這些操作可以在一個ConvNet中重複多次。事實上，今天一些表現最好的ConvNets擁有數十個Convolution和Pooling層！此外，在每個卷積層之後沒有必要具有池化層。從下面的圖16中可以看出，在進行Pooling操作之前，我們可以連續進行多次Convolution + ReLU操作。還要注意ConvNet的每一層如何在下面的圖16中可視化。\n",
    "\n",
    "![img9](https://ujwlkarn.files.wordpress.com/2016/08/car.png?w=1212)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "## <font color=\"red\">Strongly recommend reading</font>\n",
    "\n",
    "This post was originally inspired from \n",
    "\n",
    "## [Understanding Convolutional Neural Networks for NLP by Denny Britz](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/) \n",
    "\n",
    "(which I would recommend reading) and a number of explanations here are based on that post. For a more thorough understanding of some of these concepts, I would encourage you to go through the [notes](http://cs231n.github.io/) from [Stanford’s course on ConvNets](http://cs231n.stanford.edu/) as well as other excellent resources mentioned under References below. If you face any issues understanding any of the above concepts or have questions / suggestions, feel free to leave a comment below.\n",
    "\n",
    "\n",
    "\n",
    "--\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [[機器學習 ML NOTE]Convolution Neural Network 卷積神經網路](https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-convolution-neural-network-%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-bfa8566744e9)\n",
    "\n",
    "![img201908020920](https://miro.medium.com/max/700/1*Wb5a_EQyGRCC080gzof6WQ.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [Visualizing parts of Convolutional Neural Networks using Keras and Cats](https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59)\n",
    "\n",
    "## Recap\n",
    "\n",
    "Three main types of layers in CNNs: Convolutional, Pooling, Activation\n",
    "\n",
    "> <font color=\"red\">1. Convolutional layers multiply kernel value by the image window and optimize the kernel weights over time using gradient descent</font>\n",
    "\n",
    "\n",
    "> <font color=\"red\">2. Pooling layers describe a window of an image using a single value which is the max or the average of that window</font>\n",
    "\n",
    "\n",
    "> <font color=\"red\">3. Activation layers squash the values into a range, typically [0,1] or [-1,1]</font>\n",
    "\n",
    "\n",
    "\n",
    "![img20-0](https://hackernoon.com/hn-images/1*ZCjPUFrB6eHPRi4eyP6aaA.gif)\n",
    "\n",
    "![img20-1](https://hackernoon.com/hn-images/1*Feiexqhmvh9xMGVVJweXhg.gif)\n",
    "\n",
    "![img20](https://hackernoon.com/hn-images/1*rIiBaH5IMVPaE5BM-n7VZw.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img21](https://imgcrack.com/files/onxqaov5w76.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img22](https://theffork.com/wp-content/uploads/2019/02/final.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
