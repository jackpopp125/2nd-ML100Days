{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(87)\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新聞標題 (文字應用的NN)\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import Embedding  # Embedding 層將輸入序列編碼為一個稠密向量的序列\n",
    "from keras.layers import LSTM       # LSTM 層把向量序列轉換成單個向量，它包含整個序列的上下文信息\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定在訓練模型(fit model)時，所用到的參數\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "學習率太低，需要更新很多次才能到最佳解，\n",
    "學習率太高，有可能會造成梯度走不進去局部極值(但也可以擺脫局部極值的問題，等等有範例)。\n",
    "\n",
    "\n",
    "這個學習率太小，初始值不好，解就會掉到局部極小值。\n",
    "這個學習率(0.0004)對此例子來說，雖然步伐夠大跳出了局部極值，但到全域極值時，因為步伐太大，所以走不到最好的值。\n",
    "這個學習率(0.0003)對此例子來說就夠了，可以走到全域極值。\n",
    "\n",
    "\n",
    "### 學習率對梯度下降法的影響\n",
    "\n",
    "學習率較小時，收斂到正確結果的速度較慢。\n",
    "\n",
    "學習率較大時，容易在搜索過程中發生震盪。\n",
    "\n",
    "\n",
    "### Result\n",
    "\n",
    "學習率較大時，容易在搜索過程中發生震盪，而發生震盪的根本原因無非就是搜索的步長邁的太大了\n",
    "\n",
    "如果讓能夠lr隨著迭代週期不斷衰減變小，那麼搜索時邁的步長就能不斷減少以減緩震盪學習率衰減因子由此誕生\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料準備 / 取得資料，並分成 Training 與 Test set\n",
    "(x_img_train,y_label_train), (x_img_test, y_label_test) = cifar10.load_data()\n",
    "\n",
    "# 確認資料維度\n",
    "print(\"train data:\",'images:', x_img_train.shape, \" labels:\", y_label_train.shape) \n",
    "print(\"test  data:\",'images:', x_img_test.shape , \" labels:\", y_label_test.shape) \n",
    "\n",
    "\n",
    "\n",
    "# 資料(影像)標準化 (Image normalization)，並設定 data array 為浮點數\n",
    "x_img_train_normalize = x_img_train.astype('float32') / 255.0\n",
    "x_img_test_normalize = x_img_test.astype('float32') / 255.0\n",
    "\n",
    "\n",
    "\n",
    "# 將資料從圖形 (RGB) 轉為向量 (Single Vector)\n",
    "x_train = x_train.reshape((len(x_train), -1))\n",
    "x_test = x_test.reshape((len(x_test), -1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 轉換 label 為 OneHot Encoding (只要是做分類問題的時候，都要對 label 做 one-hot encoding喔！) Convert class vectors to binary class matrices.\n",
    "# 針對 Label 做 ONE HOT Encoding, 並查看維度資訊\n",
    "from keras.utils import np_utils\n",
    "y_label_train_OneHot = np_utils.to_categorical(y_label_train)\n",
    "y_label_test_OneHot = np_utils.to_categorical(y_label_test)\n",
    "print(y_label_test_OneHot.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 是否要做資料處理\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history = model.fit(x_train, \n",
    "                        y_train,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (x_test, y_test),\n",
    "                        shuffle = True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    print('')\n",
    "        \n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                # randomly shift images horizontally (fraction of total width)\n",
    "                width_shift_range=0.1,\n",
    "                # randomly shift images vertically (fraction of total height)\n",
    "                height_shift_range=0.1,\n",
    "                shear_range=0.,  # set range for random shear\n",
    "                zoom_range=0.,  # set range for random zoom\n",
    "                channel_shift_range=0.,  # set range for random channel shifts\n",
    "                # set mode for filling points outside the input boundaries\n",
    "                fill_mode='nearest',\n",
    "                cval=0.,  # value used for fill_mode = \"constant\"\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False,  # randomly flip images\n",
    "                # set rescaling factor (applied before any other transformation)\n",
    "                rescale=None,\n",
    "                # set function that will be applied on each input\n",
    "                preprocessing_function=None,\n",
    "                # image data format, either \"channels_first\" or \"channels_last\"\n",
    "                data_format=None,\n",
    "                # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "                validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    \n",
    "    \"\"\" 這個不是訓練模型 (fit model，底下的 model.fit(...) 才是！) \"\"\"\n",
    "    datagen.fit(x_train) # 這個不是訓練模型 (fit model，底下的 model.fit(...) 才是！)\n",
    "    \n",
    "    history = model.fit(x_train, \n",
    "                        y_train,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (x_test, y_test),\n",
    "                        shuffle = True)   \n",
    "\n",
    "'''\n",
    "   第四步：訓練\n",
    "   .fit的一些參數\n",
    "   batch_size：對總的樣本數進行分組，每組包含的樣本數量\n",
    "   epochs ：訓練次數\n",
    "   shuffle：是否把數據隨機打亂之後再進行訓練\n",
    "   validation_split：拿出百分之多少用來做交叉驗證\n",
    "   verbose：屏顯模式 - 0：不輸出, 1：輸出進度, 2：輸出每次的訓練結果\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (多層 Perceptron) 的優缺點\n",
    "\n",
    "MLP 優點\n",
    "\n",
    "> 建立 「非線性」 模型、 「real-time」 模型\n",
    "\n",
    "MLP 缺點\n",
    "\n",
    ">使用不同的初始權重，會讓驗證時的準確率浮動 MLP 需要調整每層神經元數、層數、迭代次數 對於特徵預處理很敏感"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "# build model\n",
    "\"\"\"\n",
    "NN的組成\n",
    "\n",
    "1. Input Layer\n",
    "\n",
    "    Weights\n",
    "\n",
    "2. Hidden Layer\n",
    "\n",
    "    Net Input Function\n",
    "\n",
    "    Activation Function : 給神經元引入非線性因素，使得 神經網路能任意逼近任何非線性函數\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### 啟動函數\n",
    "    \n",
    "    啟動函數的最⼤作⽤就是非線性化．如果不⽤啟動函數的話，無論神經網路有多少層，輸出都是輸入的線性組合\n",
    "\n",
    "    它應該是可以區分前⾏網路與反向式傳播網路的網路參數更新，然後相應地使⽤梯度下降或任何其他優化技術優化權重以減少誤差\n",
    "\n",
    "\n",
    "\n",
    "    ### 根據各個函數的優缺點來配置\n",
    "    \n",
    "    如果使⽤ ReLU，要⼩⼼設置 learning rate，注意不要讓網路出現很多「dead」 神經元，如果不好解決，可以試試 Leaky ReLU、PReLU 或者Maxout\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### 根據問題的性質\n",
    "    \n",
    "    ⽤於分類器時，Sigmoid 函數及其組合通常效果更好\n",
    "\n",
    "    由於梯度消失問題，有時要避免使⽤ sigmoid 和 tanh 函數。ReLU 函數是⼀個通⽤的啟動函數，⽬前在⼤多數情況下使⽤\n",
    "\n",
    "    如果神經網路中出現死神經元，那麼 PReLU 函數就是最好的選擇\n",
    "\n",
    "    ReLU 函數建議只能在隱藏層中使⽤\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Output Layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# model - 1\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# model - 2\"\"\"\n",
    "# 宣告採用序列模型\n",
    "model = Sequential()\n",
    "\n",
    "# 卷積層1 - filters=32\n",
    "# 與池化層1\n",
    "model.add(Conv2D(filters=32,\n",
    "                 kernel_size=(3,3),\n",
    "                 input_shape=(32, 32,3), \n",
    "                 activation='relu', \n",
    "                 padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 卷積層2 - filters=64\n",
    "# 與池化層2\n",
    "model.add(Conv2D(filters=64, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu', \n",
    "                 padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 卷積層3 - filters=128\n",
    "# 與池化層3\n",
    "model.add(Conv2D(filters=128, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu', \n",
    "                 padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 卷積層4 - filters=256\n",
    "# 與池化層4\n",
    "model.add(Conv2D(filters=256, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu', \n",
    "                 padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\"\"\"\n",
    "# 建立神經網路(平坦層、隱藏層、輸出層)\n",
    "\"\"\"\n",
    "\"\"\" 1. Flatten layer (Input Layer) \"\"\"\n",
    "model.add(Flatten())\n",
    "\n",
    "\"\"\" 2. Fully Connected Layer (Hidden Layer) \"\"\"\n",
    "# 建立全網路連接層\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "\"\"\" 3. Output Layer \"\"\"\n",
    "#建立輸出層\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#檢查model 的STACK\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入之前訓練的模型\n",
    "\n",
    "try:\n",
    "    model.load_weights(\"SaveModel/cifarCnnModel.h5\")\n",
    "    print(\"載入模型成功!繼續訓練模型\")\n",
    "except :    \n",
    "    print(\"載入模型失敗!開始訓練一個新模型\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model (模型編譯)\n",
    "\"\"\"\n",
    "# 模型編譯\n",
    "\n",
    "在訓練模型之前，您需要配置學習過程，的英文這通過compile方法完成的它接收三個參數：\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Optimizer\n",
    "\n",
    "優化器optimizer。它可以是現有優化器的字符串標識符，如rmsprop或adagrad，也可以是Optimizer類的實例。\n",
    "\n",
    "\n",
    "\n",
    "### SGD-隨機梯度下降法(stochastic gradient decent)\n",
    "\n",
    "優點：SGD 每次更新時對每個樣本進⾏梯度更新， 對於很⼤的數據集來說，可能會有相似的樣本，\n",
    "     ⽽ SGD ⼀次只進⾏⼀次更新，就沒有冗餘，⽽且比較快\n",
    "\n",
    "缺點：但是 SGD 因為更新比較頻繁，會造成 cost function 有嚴重的震盪\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Loss\n",
    "\n",
    "損失函數的損失，模型試圖最小化的目標函數它可以是現有損失函數的字符串標識符，如。categorical_crossentropy或mse，也可以是一個目標函數\n",
    "\n",
    "\n",
    "### Gradient 梯度\n",
    "在微積分裡⾯，對多元函數的參數求 ∂ 偏導數，把求得的各個參數的偏導數以向量的形式寫出來，就是梯度。\n",
    "\n",
    "最常⽤的優化算法 - 梯度下降\n",
    "⽬的：沿著⽬標函數梯度下降的⽅向搜索極⼩值（也可以沿著梯度上升的⽅向搜索極⼤值）\n",
    "要計算 Gradient Descent，考慮\n",
    "• Loss = 實際 ydata – 預測 ydata\n",
    "\n",
    "\n",
    "\n",
    "現在分析在梯度下降法中最常聽到的一句話：「梯度下降法就是朝著梯度的反方向迭代地調整參數直到收斂。」這裡的梯度就是 Δ ,而梯度的反方向就是- Δ 的符號方向-- -梯度實際上是個向量。所以這個角度來說，即使我們只有一個參數需要調整，也可以認為它是個一維的向量。整個過程你可以想像自己站在一個山坡上，準備走到山腳下（最小值的地方），於是很自然地你會考慮朝著哪個方向走，方向由 - Δ 的方向給出，而至於一次走多遠，由|α Δ |來控制。這種方式相信你應該能理解其只能找到局部最小值，而不是全局的。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Gradient Descent 梯度下降法的類型\n",
    "1. 批次梯度下降(Batch gradient descent)\n",
    "\n",
    "2. 隨機梯度下降(Stochastic gradient descent)\n",
    "\n",
    "隨機梯度下降最大的缺點在於每次更新可能並不會按照正確的方向進行，因此可以帶來優化波動(擾動)，如下圖： img3不過從另一個方面來看，隨機梯度下降所帶來的波動有個好處就是，對於類似盆地區域（即很多局部極小值點）那麼這個波動的特點可能會使得優化的方向從當前的局部極小值點跳到另一個更好的局部極小值點，這樣便可能對於非凸函數，最終收斂於一個較好的局部極值點，甚至全局極值點。 由於波動，因此會使得迭代次數（學習次數）增多，即收斂速度變慢。不過最終其會和全量梯度下降算法一樣，具有相同的收斂性，即凸函數收斂於全局極值點，非凸損失函數收斂於局部極值點。\n",
    "\n",
    "3. 小批量梯度下降(Mini-batch gradient descent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 梯度下降法的挑戰\n",
    "\n",
    "雖然梯度下降算法效果很好，並且廣泛使用，但同時其也存在一些挑戰與問題需要解決：\n",
    "\n",
    "1. 選擇一個合理的學習速率很難。如果學習速率過小，則會導致收斂速度很慢。如果學習速率過大，那麼其會阻礙收斂，即在極值點附近會振盪。\n",
    "\n",
    "2. 學習速率調整(又稱學習速率調度，Learning rate schedules)[11]試圖在每次更新過程中，改變學習速率，如退火。一般使用某種事先設定的策        略或者在每次迭代中衰減一個較小的閾值。無論哪種調整方法，都需要事先進行固定設置，這邊便無法自適應每次學習的數據集特點。\n",
    "\n",
    "3. 模型所有的參數每次更新都是使用相同的學習速率。如果數據特徵是稀疏的或者每個特徵有著不同的取值統計特徵與空間，那麼便不能在每次更新中每        個參數使用相同的學習速率，那些很少出現的特徵應該使用一個相對較大的學習速率。\n",
    "\n",
    "4. 對於非凸目標函數，容易陷入那些次優的局部極值點中，如在神經網路中。那麼如何避免呢。Dauphin指出更嚴重的問題不是局部極值點，而是鞍點 (These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions .)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### avoid local minimum\n",
    " • Item-1：在訓練神經網絡的時候，通常在訓練剛開始的時候使⽤較⼤的learning rate，隨著訓練的進⾏，我們會慢慢的減⼩ learning rate\n",
    " • Item-2：隨著 iteration 改變 Learning - decay\n",
    " • Item-3：momentum(動量)\n",
    "\n",
    "def GD(w_init, df, epochs, lr):    \n",
    "    '''  \n",
    "    梯度下降法。給定起始點與目標函數的一階導函數，求在epochs次反覆運算中x的更新值\n",
    "        :param w_init: w的init value    \n",
    "        :param df: 目標函數的一階導函數    \n",
    "        :param epochs: 反覆運算週期    \n",
    "        :param lr: 學習率    \n",
    "        :return: x在每次反覆運算後的位置   \n",
    "    ''' \n",
    "\n",
    "> ### 1. lr\n",
    "\n",
    "(1)學習率較小時，收斂到極值的速度較慢。\n",
    "\n",
    "(2)學習率較大時，容易在搜索過程中發生震盪。\n",
    "\n",
    "\n",
    "\n",
    "> ### 2. Momentum (動量)\n",
    "\n",
    "「⼀顆球從⼭上滾下來，在下坡的時候速度越來越快，遇到上坡，⽅向改變，速度下降」\n",
    "\n",
    "加入的這⼀項，可以使得梯度⽅向不變的維度上速度變快，梯度⽅向有所改變的維度上的更新速度變慢，這樣就可以加快收斂並減⼩震盪\n",
    "\n",
    "\n",
    "(1)(ß 即momentum係數，通俗的理解上面式子就是，如果上一次的momentum（即ß ）與這一次的負梯度方向是相同的，那這次下降的幅度就會加大，所以這樣做能夠達到加速收斂的過程\n",
    "\n",
    "(2)如果上一次的momentum（即ß ）與這一次的負梯度方向是相反的，那這次下降的幅度就會縮減，所以這樣做能夠達到減速收斂的過程\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> ### 3. decay (學習率衰減公式)\n",
    "\n",
    "lr_i = lr_start 1.0 / (1.0 + decay i)\n",
    "\n",
    "其中lr_i為第一迭代i時的學習率，lr_start為原始學習率，decay為一個介於[0.0, 1.0]的小數。從公式上可看出：\n",
    "\n",
    "decay越小，學習率衰減地越慢，當decay = 0時，學習率保持不變。 decay越大，學習率衰減地越快，當decay = 1時，學習率衰減最快\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Back Propadation\n",
    "\n",
    "其實他就是 Gradient Descent ， 只是在面對「百萬維」的參數量時，計算量更精簡\n",
    "\n",
    "25:45\n",
    "\n",
    "從前面往後（左邊往右）算的運算量其實非常大\n",
    "\n",
    "但如果從最後一層 output layer 的偏微分算回來，因為 Chain Rule 的關係，一切就都萬事俱備、迎刃而解\n",
    "\n",
    "這就是 Back Propadation，就是建一個反過來方向的 Neural Network 啦\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. metrics 評估指標\n",
    "\n",
    "評估標準指標。對於任何分類問題，你都希望將其設置為metrics = ['accuracy']。評估標準可以是現有的標準的字符串標識符，也可以是自定義的評估標準函數。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(train) model\n",
    "\n",
    "'''\n",
    "   第四步：訓練\n",
    "   .fit的一些參數\n",
    "   batch_size：對總的樣本數進行分組，每組包含的樣本數量\n",
    "   epochs ：訓練次數\n",
    "   shuffle：是否把數據隨機打亂之後再進行訓練\n",
    "   validation_split：拿出百分之多少用來做交叉驗證\n",
    "   verbose：屏顯模式 - 0：不輸出, 1：輸出進度, 2：輸出每次的訓練結果\n",
    "''' \n",
    "\n",
    "#模型訓練, \"Train_History\" 把訓練過程所得到的數值存起來\n",
    "train_history = model.fit(x_img_train_normalize, y_label_train_OneHot,\n",
    "                          validation_split=0.25,\n",
    "                          epochs=12, \n",
    "                          batch_size=128, \n",
    "                          verbose=1\n",
    "                         )         \n",
    "\n",
    "# [validation_split = 0.2] validation_split：在0和1之間浮動。用作驗證數據的訓練數據的比例。\n",
    "# 該模型將訓練數據的這一部分分開，不會對其進行訓練，並將在每個時期結束時評估該數據的損失和任何模型指標。\n",
    "# [batch_size]：整數或None。每個梯度更新的樣本數。指定，batch_size為128\n",
    "\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "          \n",
    "          epochs = 500, # 請將 Epoch 加到 500 個，並觀察 learning curve 的走勢\n",
    "          \n",
    "          batch_size=256, \n",
    "          validation_data=(x_test, y_test), \n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以視覺畫方式檢視訓練過程\n",
    "\n",
    "# 畫出 train acc, validation acc，以檢視是否有 overfitting / underfitting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 定義一個繪圖函數\n",
    "def show_train_history(train_acc, test_acc):\n",
    "    plt.plot(train_history.history[train_acc])\n",
    "    plt.plot(train_history.history[test_acc])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train_acc', 'val_acc'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "show_train_history('acc', 'val_acc')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "\n",
    "# 以視覺畫方式檢視訓練過程\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_loss = model.history.history[\"loss\"]\n",
    "valid_loss = model.history.history[\"val_loss\"]\n",
    "\n",
    "train_acc = model.history.history[\"acc\"]\n",
    "valid_acc = model.history.history[\"val_acc\"]\n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n",
    "plt.plot(range(len(valid_loss)), valid_loss, label=\"valid loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(train_acc)), train_acc, label=\"train accuracy\")\n",
    "plt.plot(range(len(valid_acc)), valid_acc, label=\"valid accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "#    第六步：輸出\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "# scores = model.evaluate(x_Test_normalize, y_Test_OneHot)\n",
    "\n",
    "scores = model.evaluate(x_img_test_normalize, y_label_test_OneHot)\n",
    "print()\n",
    "print('accuracy=',scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結合 compile, fit, plot, scores\n",
    "\n",
    "def check_model(loss_function) :\n",
    "        \n",
    "    model.compile(loss = loss_function, optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    #模型訓練, \"Train_History\" 把訓練過程所得到的數值存起來\n",
    "    train_history = model.fit(x_img_train_normalize, \n",
    "                              y_label_train_OneHot,\n",
    "                              validation_split=0.25,\n",
    "                              epochs=12, \n",
    "                              batch_size=128, \n",
    "                              verbose=1\n",
    "                             )         \n",
    "\n",
    "    # [validation_split = 0.2] validation_split：在0和1之間浮動。用作驗證數據的訓練數據的比例。\n",
    "    # 該模型將訓練數據的這一部分分開，不會對其進行訓練，並將在每個時期結束時評估該數據的損失和任何模型指標。\n",
    "    # [batch_size]：整數或None。每個梯度更新的樣本數。指定，batch_size為128\n",
    "\n",
    "    # 定義一個繪圖函數\n",
    "    def show_train_history(train_history, train, validation):\n",
    "        plt.plot(train_history.history[train])\n",
    "        plt.plot(train_history.history[validation])\n",
    "        plt.title('Train History')\n",
    "        plt.ylabel(train)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    # plot\n",
    "    show_train_history(train_history, 'acc', 'val_acc')\n",
    "    show_train_history(train_history, 'loss', 'val_loss')\n",
    "\n",
    "    # scores\n",
    "    scores = model.evaluate(x_img_test_normalize, y_label_test_OneHot)\n",
    "    print()\n",
    "    print('accuracy=',scores[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "#check_model(loss_function)\n",
    "check_model('categorical_crossentropy')\n",
    "check_model('mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "What is “overfitting” ?\n",
    "\n",
    "(1) Overfitting的意思就是太過追求參數完美預測出訓練數據的結果，反而導致實際預測效果不佳\n",
    "\n",
    "high variance可以理解為其有著過多的 variable\n",
    "\n",
    "\n",
    "(2) 跟overfit相反的狀況：underfit，代表在訓練數據中也有著高預測誤差的問題\n",
    "\n",
    "high bias可以理解為其會過度依賴其截距(θ0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 可能會有人好奇 λ 到底代表什麼意思？\n",
    "\n",
    "λ代表的其實是我們對於預測誤差跟正規項的取捨 !\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "當今天 λ越大，模型會比較不重視預測落差反而是極力地想要壓低所有θ值的大小 就像是我們如果把 λ 設成10¹⁰的話，所有θ值都會趨近於0，最後形成一條直線的hypothesis img7\n",
    "\n",
    "而若是 λ 越小甚至趨近於0，可能對改善overfitting就沒什麼太大的幫助了\n",
    "\n",
    "我在學習正規化的時候，對於為什麼只是加上個正規多項式就可以改善overfitting感到非常疑惑\n",
    "\n",
    "畢竟我們根本不知道要降低哪一個 θ值(feature)的影響力啊\n",
    "\n",
    "就這樣直接一視同仁的一起打壓所有的θ值到底為什麼會有效？\n",
    "\n",
    "\n",
    "\n",
    "這是我想到的答案(沒有證實過)：的確是一視同仁的打壓所有的 θ值，但是若是今天θ值只要設定到某幾個數字的話可以使預測誤差降到非常低的話，那麼模型如果為了貪小便宜的將那幾個 θ值給壓低反而造成了預測誤差的大幅上升，使得J(θ)不降反升的反效果的話，會非常得不償失，因此模型將會斟酌不要降低那些重要的 θ值\n",
    "\n",
    "上述的假設都建立在今天 λ 設立得宜的情況以及有足夠的資料來support預測落差的下降，說服模型不要把重要的 θ值降低\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "EliteDataScience - 如何減少 Overfitting 的發生\n",
    "\n",
    "1. 使用 K-fold cross validation\n",
    "    \n",
    "    找到一組參數可以在多組不同的 validation 上得到相似的結果\n",
    "\n",
    "2. 使用更多的訓練資料\n",
    "\n",
    "3. 減少 Features (參數) 的使用量 : 人工選擇、model selection algorithm\n",
    "    \n",
    "    避免參數比潛在組合更多的狀況發生，以免模型靠硬記就可以得到結果\n",
    "\n",
    "4. 在模型訓練的過程中加入正則化參數 (Regularization) : 維持現有的features，但是降低部分不重要feature的影響力。這對於有著許多feature的hypothesis很有幫助\n",
    "    \n",
    "    控制 input 的改變對模型造成的影響太大。\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "### Underfitting\n",
    "\n",
    "Error from Bias\n",
    "\n",
    "Bias is the difference between your model's expected predictions and the true values.\n",
    "\n",
    "\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Error from Variance\n",
    "\n",
    "Variance refers to your algorithm's sensitivity to specific sets of training data..\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How to Prevent Overfitting\n",
    "1. Cross-validation\n",
    "2. Train with more data\n",
    "3. Remove features\n",
    "4. Early stopping\n",
    "5. Regularization\n",
    "6. Ensembling\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
