{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(87)\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新聞標題 (文字應用的NN)\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import Embedding  # Embedding 層將輸入序列編碼為一個稠密向量的序列\n",
    "from keras.layers import LSTM       # LSTM 層把向量序列轉換成單個向量，它包含整個序列的上下文信息\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定在訓練模型(fit model)時，所用到的參數\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "學習率太低，需要更新很多次才能到最佳解，\n",
    "學習率太高，有可能會造成梯度走不進去局部極值(但也可以擺脫局部極值的問題，等等有範例)。\n",
    "\n",
    "\n",
    "這個學習率太小，初始值不好，解就會掉到局部極小值。\n",
    "這個學習率(0.0004)對此例子來說，雖然步伐夠大跳出了局部極值，但到全域極值時，因為步伐太大，所以走不到最好的值。\n",
    "這個學習率(0.0003)對此例子來說就夠了，可以走到全域極值。\n",
    "\n",
    "\n",
    "### 學習率對梯度下降法的影響\n",
    "\n",
    "學習率較小時，收斂到正確結果的速度較慢。\n",
    "\n",
    "學習率較大時，容易在搜索過程中發生震盪。\n",
    "\n",
    "\n",
    "### Result\n",
    "\n",
    "學習率較大時，容易在搜索過程中發生震盪，而發生震盪的根本原因無非就是搜索的步長邁的太大了\n",
    "\n",
    "如果讓能夠lr隨著迭代週期不斷衰減變小，那麼搜索時邁的步長就能不斷減少以減緩震盪學習率衰減因子由此誕生\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料準備 / 取得資料，並分成 Training 與 Test set\n",
    "(x_img_train,y_label_train), (x_img_test, y_label_test) = cifar10.load_data()\n",
    "\n",
    "# 確認資料維度\n",
    "print(\"train data:\",'images:', x_img_train.shape, \" labels:\", y_label_train.shape) \n",
    "print(\"test  data:\",'images:', x_img_test.shape , \" labels:\", y_label_test.shape) \n",
    "\n",
    "\n",
    "\n",
    "# 資料(影像)標準化 (Image normalization)，並設定 data array 為浮點數\n",
    "x_img_train_normalize = x_img_train.astype('float32') / 255.0\n",
    "x_img_test_normalize = x_img_test.astype('float32') / 255.0\n",
    "\n",
    "\n",
    "\n",
    "# 將資料從圖形 (RGB) 轉為向量 (Single Vector)\n",
    "x_train = x_train.reshape((len(x_train), -1))\n",
    "x_test = x_test.reshape((len(x_test), -1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 轉換 label 為 OneHot Encoding (只要是做分類問題的時候，都要對 label 做 one-hot encoding喔！) Convert class vectors to binary class matrices.\n",
    "# 針對 Label 做 ONE HOT Encoding, 並查看維度資訊\n",
    "from keras.utils import np_utils\n",
    "y_label_train_OneHot = np_utils.to_categorical(y_label_train)\n",
    "y_label_test_OneHot = np_utils.to_categorical(y_label_test)\n",
    "print(y_label_test_OneHot.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 是否要做資料處理\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history = model.fit(x_train, \n",
    "                        y_train,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (x_test, y_test),\n",
    "                        shuffle = True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    print('')\n",
    "        \n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                # randomly shift images horizontally (fraction of total width)\n",
    "                width_shift_range=0.1,\n",
    "                # randomly shift images vertically (fraction of total height)\n",
    "                height_shift_range=0.1,\n",
    "                shear_range=0.,  # set range for random shear\n",
    "                zoom_range=0.,  # set range for random zoom\n",
    "                channel_shift_range=0.,  # set range for random channel shifts\n",
    "                # set mode for filling points outside the input boundaries\n",
    "                fill_mode='nearest',\n",
    "                cval=0.,  # value used for fill_mode = \"constant\"\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False,  # randomly flip images\n",
    "                # set rescaling factor (applied before any other transformation)\n",
    "                rescale=None,\n",
    "                # set function that will be applied on each input\n",
    "                preprocessing_function=None,\n",
    "                # image data format, either \"channels_first\" or \"channels_last\"\n",
    "                data_format=None,\n",
    "                # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "                validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    \n",
    "    \"\"\" 這是 Traning Generator，不是訓練我們的預測模型(底下的 model.fit(...) 才是！) \"\"\"\n",
    "    datagen.fit(x_train) # 這是 Traning Generator，不是訓練我們的預測模型(底下的 model.fit(...) 才是！)\n",
    "    \n",
    "    history = model.fit(x_train, \n",
    "                        y_train,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (x_test, y_test),\n",
    "                        shuffle = True)   \n",
    "\n",
    "'''\n",
    "   第四步：訓練\n",
    "   .fit的一些參數\n",
    "   batch_size：對總的樣本數進行分組，每組包含的樣本數量\n",
    "   epochs ：訓練次數\n",
    "   shuffle：是否把數據隨機打亂之後再進行訓練\n",
    "   validation_split：拿出百分之多少用來做交叉驗證\n",
    "   verbose：屏顯模式 - 0：不輸出, 1：輸出進度, 2：輸出每次的訓練結果\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (多層 Perceptron) 的優缺點\n",
    "\n",
    "MLP 優點\n",
    "\n",
    "> 建立 「非線性」 模型、 「real-time」 模型\n",
    "\n",
    "MLP 缺點\n",
    "\n",
    ">使用不同的初始權重，會讓驗證時的準確率浮動 MLP 需要調整每層神經元數、層數、迭代次數 對於特徵預處理很敏感"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### - 在 Dense 裡加入 kernel_regularizer\n",
    "### - 加入不同 Layers : BatchNormalization()(x), Dropout(drp_ratio)(x)\n",
    "\n",
    "### - 在 fit 時 搭配 callback 監控\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 建立模型\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "\n",
    "# build model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "NN的組成\n",
    "\n",
    "1. Input Layer\n",
    "\n",
    "    Weights\n",
    "\n",
    "2. Hidden Layer\n",
    "\n",
    "    Net Input Function\n",
    "\n",
    "    Activation Function : 給神經元引入非線性因素，使得 神經網路能任意逼近任何非線性函數\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### 啟動函數\n",
    "    \n",
    "    啟動函數的最⼤作⽤就是非線性化．如果不⽤啟動函數的話，無論神經網路有多少層，輸出都是輸入的線性組合\n",
    "\n",
    "    它應該是可以區分前⾏網路與反向式傳播網路的網路參數更新，然後相應地使⽤梯度下降或任何其他優化技術優化權重以減少誤差\n",
    "\n",
    "\n",
    "\n",
    "    ### 根據各個函數的優缺點來配置\n",
    "    \n",
    "    如果使⽤ ReLU，要⼩⼼設置 learning rate，注意不要讓網路出現很多「dead」 神經元，如果不好解決，可以試試 Leaky ReLU、PReLU 或者Maxout\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### 根據問題的性質\n",
    "    \n",
    "    ⽤於分類器時，Sigmoid 函數及其組合通常效果更好\n",
    "\n",
    "    由於梯度消失問題，有時要避免使⽤ sigmoid 和 tanh 函數。ReLU 函數是⼀個通⽤的啟動函數，⽬前在⼤多數情況下使⽤\n",
    "\n",
    "    如果神經網路中出現死神經元，那麼 PReLU 函數就是最好的選擇\n",
    "\n",
    "    ReLU 函數建議只能在隱藏層中使⽤\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### 其中一個防止 Overfitting 的修正 : 加入 Dropout(drp_ratio)(x) Layer\n",
    "    \n",
    "\n",
    "\n",
    "    ### Activation function 缺點的修正 : 加入 BatchNormalization()(x) Layer \n",
    "    \n",
    "    「Activation function 對於極端值不敏感」這樣的特性很糟糕, (因為你看像sigmoid都縮放到[])\n",
    "    想像我輕輕拍自己的感覺和重重打自己的感覺居然沒什麼差別, 就像是我的感官系統失效了.\n",
    "    \n",
    "    \n",
    "    當然我們是可以用之前提到的對數據做normalization 預處理, 使得輸入的 x 變化範圍不會太大, 讓輸入值經過激勵函數的敏感部分. 但剛剛這個不敏感問 不僅僅發生在神經網絡的輸入層, 而且在隱藏層中也經常會發生.\n",
    "\n",
    "    Batch normalization 的 batch 是批數據, 把數據分成小批小批進行stochastic gradient descent. \n",
    "    而且在每批數據進行前向傳遞forward propagation 的時候, 對每一層都進行normalization 的處理,\n",
    "    \n",
    "    \n",
    "    沒有normalize 的數據使用tanh 激活以後, 激活值大部分都分佈到了飽和階段, 也就是大部分的激活值不是-1, 就是1, \n",
    "    而normalize 以後, 大部分的激活值在每個分佈區間都還有存在. 再將這個激活後的分佈傳遞到下一層神經網絡進行後續計算, \n",
    "    每個區間都有分佈的這一種對於神經網絡就會更加有價值\n",
    "    \n",
    "    \n",
    "    Batch Normalization 的成果 : \n",
    "    normalize 以後, 大部分的激活值在每個分佈區間都還有存在. \n",
    "    再將這個激活後的分佈傳遞到下一層神經網絡進行後續計算, 每個區間都有分佈的這一種對於神經網絡就會更加有價值\n",
    "    \n",
    "    \n",
    "\n",
    "3. Output Layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# model - 1\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# model - 2\"\"\"\n",
    "# 宣告採用序列模型\n",
    "model = Sequential()\n",
    "\n",
    "# 卷積層1 - filters=32\n",
    "# 與池化層1\n",
    "model.add(Conv2D(filters=32,\n",
    "                 kernel_size=(3,3),\n",
    "                 input_shape=(32, 32,3), \n",
    "                 activation='relu', \n",
    "                 padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 卷積層2 - filters=64\n",
    "# 與池化層2\n",
    "model.add(Conv2D(filters=64, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu', \n",
    "                 padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 卷積層3 - filters=128\n",
    "# 與池化層3\n",
    "model.add(Conv2D(filters=128, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu', \n",
    "                 padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 卷積層4 - filters=256\n",
    "# 與池化層4\n",
    "model.add(Conv2D(filters=256, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu', \n",
    "                 padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\"\"\"\n",
    "# 建立神經網路(平坦層、隱藏層、輸出層)\n",
    "\"\"\"\n",
    "\"\"\" 1. Flatten layer (Input Layer) \"\"\"\n",
    "model.add(Flatten())\n",
    "\n",
    "\"\"\" 2. Fully Connected Layer (Hidden Layer) \"\"\"\n",
    "# 建立全網路連接層\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "\"\"\" 3. Output Layer \"\"\"\n",
    "#建立輸出層\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#檢查model 的STACK\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "詳見 Day_084_HW - 看過 ANS 後改一版\n",
    "\n",
    "https://nbviewer.jupyter.org/github/jshuang0520/2nd-ML100Days/blob/master/homework/Day_084_HW/Day_084_HW.ipynb\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import itertools\n",
    "# Disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "\n",
    "train, test = keras.datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "## 資料前處理\n",
    "def preproc_x(x, flatten=True):\n",
    "    x = x / 255.\n",
    "    if flatten:\n",
    "        x = x.reshape((len(x), -1))\n",
    "    return x\n",
    "\n",
    "def preproc_y(y, num_classes=10):\n",
    "    if y.shape[-1] == 1:\n",
    "        y = keras.utils.to_categorical(y, num_classes)\n",
    "    return y    \n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "\n",
    "# Preproc the inputs\n",
    "x_train = preproc_x(x_train)\n",
    "x_test = preproc_x(x_test)\n",
    "\n",
    "# Preprc the outputs\n",
    "y_train = preproc_y(y_train)\n",
    "y_test = preproc_y(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Regularizers : 隱藏層內的 L1 / L2 正規化\n",
    "Dropout : 隨機省略神經元輸出的正規化\n",
    "Batch-normalization : 傳遞時神經元橫向平衡的正規化\n",
    "\"\"\"\n",
    "# from keras.layers import BatchNormalization, Activation, Dropout, regularizers\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128], use_bn=True, drp_ratio=0, l2_ratio=0):\n",
    "    \n",
    "    # input layer\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "    \n",
    "    # hidden layer\n",
    "    for i, n_units in enumerate(num_neurons):\n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(units=n_units, \n",
    "                                   activation=\"relu\", \n",
    "                                   name=\"hidden_layer\"+str(i+1), \n",
    "                                   kernel_regularizer=l2(l2_ratio))(input_layer) # regularization\n",
    "            \n",
    "            x = Dropout(drp_ratio)(x) # Dropput\n",
    "            if use_bn:\n",
    "                x = BatchNormalization()(x) # BatchNormalization\n",
    "        else:\n",
    "            x = keras.layers.Dense(units=n_units, \n",
    "                                   activation=\"relu\", \n",
    "                                   name=\"hidden_layer\"+str(i+1),\n",
    "                                   kernel_regularizer=l2(l2_ratio))(x) # regularization\n",
    "            x = Dropout(drp_ratio)(x) # Dropput\n",
    "            if use_bn:\n",
    "                x = BatchNormalization()(x) # BatchNormalization\n",
    "    \n",
    "    # output layer\n",
    "    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 超參數設定\n",
    "\"\"\"\n",
    "Set your hyper-parameters\n",
    "\"\"\"\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "MOMENTUM = 0.95\n",
    "\n",
    "\"\"\"\n",
    "建立實驗組合\n",
    "\"\"\"\n",
    "USE_BN = [True, False]\n",
    "DRP_RATIO = [0, 0.25, 0.5, 0.8]\n",
    "L2_RATIO = [0, 1e-2, 1e-4]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collect results\n",
    "import keras.backend as K\n",
    "\n",
    "\"\"\"\n",
    "以迴圈方式遍歷組合來訓練模型\n",
    "\"\"\"\n",
    "results = {}\n",
    "for i, (use_bn, drp_ratio, l2_ratio) in enumerate(itertools.product(USE_BN, DRP_RATIO, L2_RATIO)):\n",
    "    \n",
    "    # clear_session\n",
    "    K.clear_session()\n",
    "    \n",
    "    # build model\n",
    "    print(\"Numbers of exp: %i, with bn: %s, drp_ratio: %.2f, l2_ratio: %.2f\" % (i, use_bn, drp_ratio, l2_ratio))\n",
    "    model = build_mlp(input_shape=x_train.shape[1:], use_bn=use_bn, drp_ratio=drp_ratio, l2_ratio=l2_ratio)\n",
    "    model.summary()\n",
    "    \n",
    "    # compile\n",
    "#   optimizer = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
    "\n",
    "    # fit\n",
    "    model.fit(x_train, y_train, \n",
    "              epochs=EPOCHS, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              validation_data=(x_test, y_test), \n",
    "              verbose=1,\n",
    "              shuffle=True)\n",
    "    \n",
    "    # Collect results\n",
    "    exp_name_tag = (\"exp-%s\" % (i))\n",
    "    results[exp_name_tag] = {'train-loss': model.history.history[\"loss\"],\n",
    "                             'valid-loss': model.history.history[\"val_loss\"],\n",
    "                             'train-acc': model.history.history[\"acc\"],\n",
    "                             'valid-acc': model.history.history[\"val_acc\"]}\n",
    "\n",
    "\n",
    "\n",
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as mplcm\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "\n",
    "# 顏色數 要和前面的 嘗試參數組合數 一樣\n",
    "NUM_COLORS = 24\n",
    "cm = plt.get_cmap('gist_rainbow')\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=NUM_COLORS-1)\n",
    "scalarMap = mplcm.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "color_bar = [scalarMap.to_rgba(i) for i in range(NUM_COLORS)]\n",
    "\n",
    "# plot\n",
    "# loss\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cond in enumerate(results.keys()):\n",
    "    plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n",
    "    plt.plot(range(len(results[cond]['valid-loss'])),results[cond]['valid-loss'], '--', label=cond, color=color_bar[i])\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "# acc\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cond in enumerate(results.keys()):\n",
    "    plt.plot(range(len(results[cond]['train-acc'])),results[cond]['train-acc'], '-', label=cond, color=color_bar[i])\n",
    "    plt.plot(range(len(results[cond]['valid-acc'])),results[cond]['valid-acc'], '--', label=cond, color=color_bar[i])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入之前訓練的模型\n",
    "\n",
    "try:\n",
    "    model.load_weights(\"SaveModel/cifarCnnModel.h5\")\n",
    "    print(\"載入模型成功!繼續訓練模型\")\n",
    "except :    \n",
    "    print(\"載入模型失敗!開始訓練一個新模型\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model (模型編譯)\n",
    "\"\"\"\n",
    "# 模型編譯\n",
    "\n",
    "在訓練模型之前，您需要配置學習過程，的英文這通過compile方法完成的它接收三個參數：\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Optimizer\n",
    "\n",
    "優化器optimizer。它可以是現有優化器的字符串標識符，如rmsprop或adagrad，也可以是Optimizer類的實例。\n",
    "\n",
    "\n",
    "\n",
    "### SGD-隨機梯度下降法(stochastic gradient decent)\n",
    "\n",
    "優點：SGD 每次更新時對每個樣本進⾏梯度更新， 對於很⼤的數據集來說，可能會有相似的樣本，\n",
    "     ⽽ SGD ⼀次只進⾏⼀次更新，就沒有冗餘，⽽且比較快\n",
    "\n",
    "缺點：但是 SGD 因為更新比較頻繁，會造成 cost function 有嚴重的震盪\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Optimizer 種類 \n",
    "optimizer = keras.optimizers.SGD(lr, nesterov, momentum)\n",
    "\n",
    "\n",
    "SGD\n",
    "keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "\n",
    "RMSprop\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "\n",
    "AdaGrad\n",
    "keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "\n",
    "Adam\n",
    "keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "\n",
    "\n",
    "詳見 Day_080_HW 最下方參考閱讀的筆記\n",
    "https://nbviewer.jupyter.org/github/jshuang0520/2nd-ML100Days/blob/master/homework/Day_080_HW/Day_080_HW.ipynb \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Loss\n",
    "\n",
    "損失函數的損失，模型試圖最小化的目標函數它可以是現有損失函數的字符串標識符，如。categorical_crossentropy或mse，也可以是一個目標函數\n",
    "\n",
    "\n",
    "### Gradient 梯度\n",
    "在微積分裡⾯，對多元函數的參數求 ∂ 偏導數，把求得的各個參數的偏導數以向量的形式寫出來，就是梯度。\n",
    "\n",
    "最常⽤的優化算法 - 梯度下降\n",
    "⽬的：沿著⽬標函數梯度下降的⽅向搜索極⼩值（也可以沿著梯度上升的⽅向搜索極⼤值）\n",
    "要計算 Gradient Descent，考慮\n",
    "• Loss = 實際 ydata – 預測 ydata\n",
    "\n",
    "\n",
    "\n",
    "現在分析在梯度下降法中最常聽到的一句話：「梯度下降法就是朝著梯度的反方向迭代地調整參數直到收斂。」這裡的梯度就是 Δ ,而梯度的反方向就是- Δ 的符號方向-- -梯度實際上是個向量。所以這個角度來說，即使我們只有一個參數需要調整，也可以認為它是個一維的向量。整個過程你可以想像自己站在一個山坡上，準備走到山腳下（最小值的地方），於是很自然地你會考慮朝著哪個方向走，方向由 - Δ 的方向給出，而至於一次走多遠，由|α Δ |來控制。這種方式相信你應該能理解其只能找到局部最小值，而不是全局的。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Gradient Descent 梯度下降法的類型\n",
    "1. 批次梯度下降(Batch gradient descent)\n",
    "\n",
    "2. 隨機梯度下降(Stochastic gradient descent)\n",
    "\n",
    "隨機梯度下降最大的缺點在於每次更新可能並不會按照正確的方向進行，因此可以帶來優化波動(擾動)，如下圖： img3不過從另一個方面來看，隨機梯度下降所帶來的波動有個好處就是，對於類似盆地區域（即很多局部極小值點）那麼這個波動的特點可能會使得優化的方向從當前的局部極小值點跳到另一個更好的局部極小值點，這樣便可能對於非凸函數，最終收斂於一個較好的局部極值點，甚至全局極值點。 由於波動，因此會使得迭代次數（學習次數）增多，即收斂速度變慢。不過最終其會和全量梯度下降算法一樣，具有相同的收斂性，即凸函數收斂於全局極值點，非凸損失函數收斂於局部極值點。\n",
    "\n",
    "3. 小批量梯度下降(Mini-batch gradient descent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 梯度下降法的挑戰\n",
    "\n",
    "雖然梯度下降算法效果很好，並且廣泛使用，但同時其也存在一些挑戰與問題需要解決：\n",
    "\n",
    "1. 選擇一個合理的學習速率很難。如果學習速率過小，則會導致收斂速度很慢。如果學習速率過大，那麼其會阻礙收斂，即在極值點附近會振盪。\n",
    "\n",
    "2. 學習速率調整(又稱學習速率調度，Learning rate schedules)[11]試圖在每次更新過程中，改變學習速率，如退火。一般使用某種事先設定的策        略或者在每次迭代中衰減一個較小的閾值。無論哪種調整方法，都需要事先進行固定設置，這邊便無法自適應每次學習的數據集特點。\n",
    "\n",
    "3. 模型所有的參數每次更新都是使用相同的學習速率。如果數據特徵是稀疏的或者每個特徵有著不同的取值統計特徵與空間，那麼便不能在每次更新中每        個參數使用相同的學習速率，那些很少出現的特徵應該使用一個相對較大的學習速率。\n",
    "\n",
    "4. 對於非凸目標函數，容易陷入那些次優的局部極值點中，如在神經網路中。那麼如何避免呢。Dauphin指出更嚴重的問題不是局部極值點，而是鞍點 (These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions .)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### avoid local minimum\n",
    " • Item-1：在訓練神經網絡的時候，通常在訓練剛開始的時候使⽤較⼤的learning rate，隨著訓練的進⾏，我們會慢慢的減⼩ learning rate\n",
    " • Item-2：隨著 iteration 改變 Learning - decay\n",
    " • Item-3：momentum(動量)\n",
    "\n",
    "def GD(w_init, df, epochs, lr):    \n",
    "    '''  \n",
    "    梯度下降法。給定起始點與目標函數的一階導函數，求在epochs次反覆運算中x的更新值\n",
    "        :param w_init: w的init value    \n",
    "        :param df: 目標函數的一階導函數    \n",
    "        :param epochs: 反覆運算週期    \n",
    "        :param lr: 學習率    \n",
    "        :return: x在每次反覆運算後的位置   \n",
    "    ''' \n",
    "\n",
    "> ### 1. lr\n",
    "\n",
    "(1)學習率較小時，收斂到極值的速度較慢。\n",
    "\n",
    "(2)學習率較大時，容易在搜索過程中發生震盪。\n",
    "\n",
    "\n",
    "\n",
    "> ### 2. Momentum (動量)\n",
    "\n",
    "「⼀顆球從⼭上滾下來，在下坡的時候速度越來越快，遇到上坡，⽅向改變，速度下降」\n",
    "\n",
    "加入的這⼀項，可以使得梯度⽅向不變的維度上速度變快，梯度⽅向有所改變的維度上的更新速度變慢，這樣就可以加快收斂並減⼩震盪\n",
    "\n",
    "\n",
    "(1)(ß 即momentum係數，通俗的理解上面式子就是，如果上一次的momentum（即ß ）與這一次的負梯度方向是相同的，那這次下降的幅度就會加大，所以這樣做能夠達到加速收斂的過程\n",
    "\n",
    "(2)如果上一次的momentum（即ß ）與這一次的負梯度方向是相反的，那這次下降的幅度就會縮減，所以這樣做能夠達到減速收斂的過程\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> ### 3. decay (學習率衰減公式)\n",
    "\n",
    "lr_i = lr_start 1.0 / (1.0 + decay i)\n",
    "\n",
    "其中lr_i為第一迭代i時的學習率，lr_start為原始學習率，decay為一個介於[0.0, 1.0]的小數。從公式上可看出：\n",
    "\n",
    "decay越小，學習率衰減地越慢，當decay = 0時，學習率保持不變。 decay越大，學習率衰減地越快，當decay = 1時，學習率衰減最快\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Back Propadation\n",
    "\n",
    "其實他就是 Gradient Descent ， 只是在面對「百萬維」的參數量時，計算量更精簡\n",
    "\n",
    "25:45\n",
    "\n",
    "從前面往後（左邊往右）算的運算量其實非常大\n",
    "\n",
    "但如果從最後一層 output layer 的偏微分算回來，因為 Chain Rule 的關係，一切就都萬事俱備、迎刃而解\n",
    "\n",
    "這就是 Back Propadation，就是建一個反過來方向的 Neural Network 啦\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. metrics 評估指標\n",
    "\n",
    "評估標準指標。對於任何分類問題，你都希望將其設置為metrics = ['accuracy']。評估標準可以是現有的標準的字符串標識符，也可以是自定義的評估標準函數。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(train) model\n",
    "\n",
    "'''\n",
    "   第四步：訓練\n",
    "   .fit的一些參數\n",
    "   batch_size：對總的樣本數進行分組，每組包含的樣本數量\n",
    "   epochs ：訓練次數\n",
    "   shuffle：是否把數據隨機打亂之後再進行訓練\n",
    "   validation_split：拿出百分之多少用來做交叉驗證\n",
    "   verbose：屏顯模式 - 0：不輸出, 1：輸出進度, 2：輸出每次的訓練結果\n",
    "''' \n",
    "\n",
    "#模型訓練, \"Train_History\" 把訓練過程所得到的數值存起來\n",
    "train_history = model.fit(x_img_train_normalize, y_label_train_OneHot,\n",
    "                          validation_split=0.25,\n",
    "                          epochs=12, \n",
    "                          batch_size=128, \n",
    "                          verbose=1\n",
    "                         )         \n",
    "\n",
    "# [validation_split = 0.2] validation_split：在0和1之間浮動。用作驗證數據的訓練數據的比例。\n",
    "# 該模型將訓練數據的這一部分分開，不會對其進行訓練，並將在每個時期結束時評估該數據的損失和任何模型指標。\n",
    "# [batch_size]：整數或None。每個梯度更新的樣本數。指定，batch_size為128\n",
    "\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "          \n",
    "          epochs = 500, # 請將 Epoch 加到 500 個，並觀察 learning curve 的走勢\n",
    "          \n",
    "          batch_size=256, \n",
    "          validation_data=(x_test, y_test), \n",
    "          shuffle=True)\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "\"\"\"\n",
    "在 fit 時 搭配 callback 監控\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# 載入 Callbacks, 並將 monitor 設定為監控 validation loss\n",
    "\"\"\"\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\"\"\"# 如'acc','val_acc','loss'和'val_loss'等等。\"\"\"\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\", \n",
    "                          patience=5, \n",
    "                          verbose=1\n",
    "                          )\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          validation_data=(x_test, y_test), \n",
    "          shuffle=True,\n",
    "          callbacks=[earlystop] # 在 fit 時 搭配 callback 監控\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以視覺化方式檢視訓練過程\n",
    "\n",
    "# 畫出 train acc, validation acc，以檢視是否有 overfitting / underfitting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 定義一個繪圖函數\n",
    "def show_train_history(train_acc, test_acc):\n",
    "    plt.plot(train_history.history[train_acc])\n",
    "    plt.plot(train_history.history[test_acc])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train_acc', 'val_acc'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "show_train_history('acc', 'val_acc')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "\n",
    "# 以視覺化方式檢視訓練過程\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_loss = model.history.history[\"loss\"]\n",
    "valid_loss = model.history.history[\"val_loss\"]\n",
    "\n",
    "train_acc = model.history.history[\"acc\"]\n",
    "valid_acc = model.history.history[\"val_acc\"]\n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n",
    "plt.plot(range(len(valid_loss)), valid_loss, label=\"valid loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(train_acc)), train_acc, label=\"train accuracy\")\n",
    "plt.plot(range(len(valid_acc)), valid_acc, label=\"valid accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "#    第六步：輸出\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "# scores = model.evaluate(x_Test_normalize, y_Test_OneHot)\n",
    "\n",
    "scores = model.evaluate(x_img_test_normalize, y_label_test_OneHot)\n",
    "print()\n",
    "print('accuracy=',scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結合 compile, fit, plot, scores\n",
    "\n",
    "def check_model(loss_function) :\n",
    "        \n",
    "    model.compile(loss = loss_function, optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    #模型訓練, \"Train_History\" 把訓練過程所得到的數值存起來\n",
    "    train_history = model.fit(x_img_train_normalize, \n",
    "                              y_label_train_OneHot,\n",
    "                              validation_split=0.25,\n",
    "                              epochs=12, \n",
    "                              batch_size=128, \n",
    "                              verbose=1\n",
    "                             )         \n",
    "\n",
    "    # [validation_split = 0.2] validation_split：在0和1之間浮動。用作驗證數據的訓練數據的比例。\n",
    "    # 該模型將訓練數據的這一部分分開，不會對其進行訓練，並將在每個時期結束時評估該數據的損失和任何模型指標。\n",
    "    # [batch_size]：整數或None。每個梯度更新的樣本數。指定，batch_size為128\n",
    "\n",
    "    # 定義一個繪圖函數\n",
    "    def show_train_history(train_history, train, validation):\n",
    "        plt.plot(train_history.history[train])\n",
    "        plt.plot(train_history.history[validation])\n",
    "        plt.title('Train History')\n",
    "        plt.ylabel(train)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    # plot\n",
    "    show_train_history(train_history, 'acc', 'val_acc')\n",
    "    show_train_history(train_history, 'loss', 'val_loss')\n",
    "\n",
    "    # scores\n",
    "    scores = model.evaluate(x_img_test_normalize, y_label_test_OneHot)\n",
    "    print()\n",
    "    print('accuracy=',scores[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "#check_model(loss_function)\n",
    "check_model('categorical_crossentropy')\n",
    "check_model('mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 超參數設定\n",
    "LEARNING_RATE = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "MOMENTUM = 0.95\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = {}\n",
    "\"\"\"\n",
    "使用迴圈，建立不同 Learning rate 的模型並訓練\n",
    "\"\"\"\n",
    "for lr in LEARNING_RATE:\n",
    "    keras.backend.clear_session() # 把舊的 Graph 清掉\n",
    "    print(\"Experiment with LR = %.6f\" % (lr))\n",
    "    model = build_mlp(input_shape=x_train.shape[1:])\n",
    "    model.summary()\n",
    "    optimizer = keras.optimizers.SGD(lr=lr, nesterov=True, momentum=MOMENTUM)\n",
    "    # compile\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
    "    # fit\n",
    "    model.fit(x_train, y_train, \n",
    "              epochs=EPOCHS, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              validation_data=(x_test, y_test), \n",
    "              shuffle=True)\n",
    "    \n",
    "    # Collect results\n",
    "    train_loss = model.history.history[\"loss\"]\n",
    "    valid_loss = model.history.history[\"val_loss\"]\n",
    "    train_acc = model.history.history[\"acc\"]\n",
    "    valid_acc = model.history.history[\"val_acc\"]\n",
    "    \n",
    "    exp_name_tag = \"exp-lr-%s\" % str(lr)\n",
    "    results[exp_name_tag] = {'train-loss': train_loss,\n",
    "                             'valid-loss': valid_loss,\n",
    "                             'train-acc': train_acc,\n",
    "                             'valid-acc': valid_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 超參數設定\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "MOMENTUM = 0.95\n",
    "L2_EXP = [1e-2, 1e-4, 1e-8, 1e-12]\n",
    "\n",
    "\n",
    "results = {}\n",
    "\"\"\"\n",
    "使用迴圈建立不同的帶不同 L1/L2 的模型並訓練\n",
    "\"\"\"\n",
    "for regulizer_ratio in L2_EXP:\n",
    "    keras.backend.clear_session() # 把舊的 Graph 清掉\n",
    "    print(\"Experiment with Regulizer = %.6f\" % (regulizer_ratio))\n",
    "    model = build_mlp(input_shape=x_train.shape[1:], l2_ratio=regulizer_ratio)\n",
    "    model.summary()\n",
    "    optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
    "\n",
    "    model.fit(x_train, y_train, \n",
    "              epochs=EPOCHS, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              validation_data=(x_test, y_test), \n",
    "              shuffle=True)\n",
    "    \n",
    "    # Collect results\n",
    "    train_loss = model.history.history[\"loss\"]\n",
    "    valid_loss = model.history.history[\"val_loss\"]\n",
    "    train_acc = model.history.history[\"acc\"]\n",
    "    valid_acc = model.history.history[\"val_acc\"]\n",
    "    \n",
    "    exp_name_tag = \"exp-l2-%s\" % str(regulizer_ratio)\n",
    "    results[exp_name_tag] = {'train-loss': train_loss,\n",
    "                             'valid-loss': valid_loss,\n",
    "                             'train-acc': train_acc,\n",
    "                             'valid-acc': valid_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Job\n"
     ]
    }
   ],
   "source": [
    "# Tuning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "What is “overfitting” ?\n",
    "\n",
    "(1) Overfitting的意思就是太過追求參數完美預測出訓練數據的結果，反而導致實際預測效果不佳\n",
    "\n",
    "high variance可以理解為其有著過多的 variable\n",
    "\n",
    "\n",
    "(2) 跟overfit相反的狀況：underfit，代表在訓練數據中也有著高預測誤差的問題\n",
    "\n",
    "high bias可以理解為其會過度依賴其截距(θ0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 可能會有人好奇 λ 到底代表什麼意思？\n",
    "\n",
    "λ代表的其實是我們對於預測誤差跟正規項的取捨 !\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "當今天 λ越大，模型會比較不重視預測落差反而是極力地想要壓低所有θ值的大小 就像是我們如果把 λ 設成10¹⁰的話，所有θ值都會趨近於0，最後形成一條直線的hypothesis img7\n",
    "\n",
    "而若是 λ 越小甚至趨近於0，可能對改善overfitting就沒什麼太大的幫助了\n",
    "\n",
    "我在學習正規化的時候，對於為什麼只是加上個正規多項式就可以改善overfitting感到非常疑惑\n",
    "\n",
    "畢竟我們根本不知道要降低哪一個 θ值(feature)的影響力啊\n",
    "\n",
    "就這樣直接一視同仁的一起打壓所有的θ值到底為什麼會有效？\n",
    "\n",
    "\n",
    "\n",
    "這是我想到的答案(沒有證實過)：的確是一視同仁的打壓所有的 θ值，但是若是今天θ值只要設定到某幾個數字的話可以使預測誤差降到非常低的話，那麼模型如果為了貪小便宜的將那幾個 θ值給壓低反而造成了預測誤差的大幅上升，使得J(θ)不降反升的反效果的話，會非常得不償失，因此模型將會斟酌不要降低那些重要的 θ值\n",
    "\n",
    "上述的假設都建立在今天 λ 設立得宜的情況以及有足夠的資料來support預測落差的下降，說服模型不要把重要的 θ值降低\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "EliteDataScience - 如何減少 Overfitting 的發生\n",
    "\n",
    "1. 使用 K-fold cross validation\n",
    "    \n",
    "    找到一組參數可以在多組不同的 validation 上得到相似的結果\n",
    "\n",
    "2. 使用更多的訓練資料\n",
    "\n",
    "3. 減少 Features (參數) 的使用量 : 人工選擇、model selection algorithm\n",
    "    \n",
    "    避免參數比潛在組合更多的狀況發生，以免模型靠硬記就可以得到結果\n",
    "\n",
    "4. 在模型訓練的過程中加入正則化參數 (Regularization) : 維持現有的features，但是降低部分不重要feature的影響力。這對於有著許多feature的hypothesis很有幫助\n",
    "    \n",
    "    控制 input 的改變對模型造成的影響太大。\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "### Underfitting\n",
    "\n",
    "Error from Bias\n",
    "\n",
    "Bias is the difference between your model's expected predictions and the true values.\n",
    "\n",
    "\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Error from Variance\n",
    "\n",
    "Variance refers to your algorithm's sensitivity to specific sets of training data..\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How to Prevent Overfitting\n",
    "1. Cross-validation\n",
    "2. Train with more data\n",
    "3. Remove features\n",
    "4. Early stopping （當驗證集上的效果變差的時候）\n",
    "5. Regularization\n",
    "6. Ensembling\n",
    "\n",
    "7. Dropout Layer\n",
    "\n",
    "\n",
    "dropout是指在深度學習網絡的訓練過程中，對於神經網絡單元，按照一定的概率將其暫時從網絡中丟棄。注意是暫時，對於隨機梯度下降來說，由於是隨機丟棄，故而每一個mini-batch都在訓練不同的網絡。\n",
    "該論文從神經網絡的難題出發，一步一步引出dropout為何有效的解釋。大規模的神經網絡有兩個缺點：\n",
    "\n",
    "1.費時\n",
    "\n",
    "2.容易過擬合\n",
    "\n",
    "這兩個缺點真是抱在深度學習大腿上的兩個大包袱，一左一右，相得益彰，額不，臭氣相投。過擬合是很多機器學習的通病，過擬合了，得到的模型基本就廢了。而為了解決過擬合問題，一般會採用ensemble方法，即訓練多個模型做組合， 此時，費時就成為一個大問題，不僅訓練起來費時，測試起來多個模型也很費時。總之，幾乎形成了一個死鎖。\n",
    "\n",
    "Dropout的出現很好的可以解決這個問題，每次做完dropout，相當於從原始的網絡中找到一個更瘦的網絡\n",
    "\n",
    "\n",
    "\n",
    "dropout自己雖然也很厲害，但是 dropout、max-normalization、large decaying learning rates and high momentum 組合起來效果更好，\n",
    "比如 max-norm regularization 就可以防止大的 learning rate 導致的參數 blow up。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8. BatchNormalization Layer\n",
    "\n",
    "「Activation function 對於極端值不敏感」這樣的特性很糟糕, 想像我輕輕拍自己的感覺和重重打自己的感覺居然沒什麼差別, 就像是我的感官系統失效了.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"Good Job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "詳見 Day_085_HW - Work\n",
    "\n",
    "https://nbviewer.jupyter.org/github/jshuang0520/2nd-ML100Days/blob/master/homework/Day_085_HW/Day_085_HW.ipynb\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import keras\n",
    "\n",
    "# 本範例不需使用 GPU, 將 GPU 設定為 \"無\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train, test = keras.datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 資料前處理\n",
    "def preproc_x(x, flatten=True):\n",
    "    x = x / 255.\n",
    "    if flatten:\n",
    "        x = x.reshape((len(x), -1))\n",
    "    return x\n",
    "\n",
    "def preproc_y(y, num_classes=10):\n",
    "    if y.shape[-1] == 1:\n",
    "        y = keras.utils.to_categorical(y, num_classes)\n",
    "    return y    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "\n",
    "# 資料前處理 - X 標準化\n",
    "x_train = preproc_x(x_train)\n",
    "x_test = preproc_x(x_test)\n",
    "\n",
    "# 資料前處理 -Y 轉成 onehot\n",
    "y_train = preproc_y(y_train)\n",
    "y_test = preproc_y(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "\"\"\"\n",
    "建立神經網路，並加入 BN layer\n",
    "\"\"\"\n",
    "def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128]):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "    \n",
    "    for i, n_units in enumerate(num_neurons):\n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(units=n_units, \n",
    "                                   activation=\"relu\", \n",
    "                                   name=\"hidden_layer\"+str(i+1))(input_layer)\n",
    "            x = BatchNormalization()(x)\n",
    "        else:\n",
    "            x = keras.layers.Dense(units=n_units, \n",
    "                                   activation=\"relu\", \n",
    "                                   name=\"hidden_layer\"+str(i+1))(x)\n",
    "            x = BatchNormalization()(x)\n",
    "    \n",
    "    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 超參數設定\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "MOMENTUM = 0.95\n",
    "PATIENCE = [10, 15, 20, 25]\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.callbacks import EarlyStopping\n",
    "\"\"\"\n",
    "# 載入 Callbacks, 並將 monitor 設定為監控 validation loss\n",
    "\"\"\"\n",
    "# Define results\n",
    "results = {}\n",
    "for patience in PATIENCE :\n",
    "\n",
    "\n",
    "    # 如'acc','val_acc','loss'和'val_loss'等等。\n",
    "    earlystop = EarlyStopping(monitor = \"val_acc\",    # 試改變 monitor \"Validation Accuracy\" 並比較結果\n",
    "                              patience = patience,    # 調整 earlystop 的等待次數至 10, 25 並比較結果\n",
    "                              verbose=1\n",
    "                              )\n",
    "\n",
    "\n",
    "    model = build_mlp(input_shape=x_train.shape[1:])\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
    "\n",
    "    model.fit(x_train, y_train, \n",
    "              epochs=EPOCHS, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              validation_data=(x_test, y_test), \n",
    "              shuffle=True,\n",
    "              callbacks=[earlystop]\n",
    "             )\n",
    "\n",
    "    # Collect results\n",
    "    train_loss = model.history.history[\"loss\"]\n",
    "    valid_loss = model.history.history[\"val_loss\"]\n",
    "    train_acc = model.history.history[\"acc\"]\n",
    "    valid_acc = model.history.history[\"val_acc\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # plot\n",
    "    plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n",
    "    plt.plot(range(len(valid_loss)), valid_loss, label=\"valid loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(len(train_acc)), train_acc, label=\"train accuracy\")\n",
    "    plt.plot(range(len(valid_acc)), valid_acc, label=\"valid accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
