{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "\n",
    "了解線性回歸的模型發展歷程，並了解優勢與劣勢，以及其使用情境\n",
    "\n",
    "\n",
    "## 作業\n",
    "\n",
    "請閱讀以下相關文獻，並回答以下問題\n",
    "\n",
    "1. [Linear Regression 詳細介紹](https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_linear_regression_works.html)\n",
    "\n",
    "2. [Logistics Regression 詳細介紹](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-3%E8%AC%9B-%E7%B7%9A%E6%80%A7%E5%88%86%E9%A1%9E-%E9%82%8F%E8%BC%AF%E6%96%AF%E5%9B%9E%E6%AD%B8-logistic-regression-%E4%BB%8B%E7%B4%B9-a1a5f47017e5)\n",
    "\n",
    "3. [你可能不知道的 Logistic Regression](https://taweihuang.hpd.io/2017/12/22/logreg101/)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 線性回歸模型能夠準確預測非線性關係的資料集嗎?\n",
    "\n",
    "> ### 「找出符合資料規律的直線，就叫線性迴歸」的原文為 Finding the curve that best fits your data is called regression, and when that curve is a straight line, it's called linear regression. 雖然我以前學的說法是「只要因變量為自變量的線性組合，就可以稱作線性迴歸」，這代表就算線條不是直的，也可能是線性迴歸；但如果將包括多項式的迴歸細分作 polynomial regression，將作者說法算作 simple linear regression 也沒錯。\n",
    "> ### 由「只要因變量為自變量的線性組合，就可以稱作線性迴歸，這代表就算線條不是直的，也可能是線性迴歸」的觀點出發，廣義來說，是可以預測「非線性關係」的資料集的\n",
    "\n",
    "\n",
    "## 2. 回歸模型是否對資料分布有基本假設?\n",
    "\n",
    "狹義的簡單線性回歸模型有以下假設：\n",
    "\n",
    "> ### Yi = {β0 + β1*Xi}(常數；Xi非r.v) + {εi}(r.v) = {E(Yi)}(常數；Xi非r.v) + {εi}(r.v), i = 1,2,...,n\n",
    "\n",
    "> ### Assumptions : εi ~iid N(0, σ^2) --> 總之假設就是專注在期望值和變異數\n",
    "\n",
    "\n",
    "\n",
    "> ### 1. 變數 Xi 與 Yi之間的函數關係形式設定是正確的\n",
    "> ### 2. 變數 Xi 為非隨機的（非r.v），可視為在實驗設計中，事先給定的數值\n",
    "> ### 3. E(εi) = 0\n",
    "> ### 4. 同質變異數假設(homoscedesticity) : 無論解釋變數 Xi 為何， εi 的變異數都相等\n",
    "> ### 5. 序列獨立(serial independence)\n",
    "> ### 6. 常態性(normality)假設 : εi ~ Normal Distribution\n",
    "> ### 7. 綜合以上 ： εi ~iid N(0, σ^2) => E(Yi) = β0 + β1*Xi, Var(Yi) = σ^2 => Yi ~ind N(β0 + β1*Xi, σ^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考資料\n",
    "---\n",
    "\n",
    "## [超人氣 Stanford 教授 Andrew Ng 教你 Linear regression (強烈推薦觀看) ](https://zh-tw.coursera.org/lecture/machine-learning/model-representation-db3jS)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Logistic regression 數學原理](https://blog.csdn.net/qq_23269761/article/details/81778585)\n",
    "\n",
    "![img1](https://img-blog.csdn.net/20180817153816524?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIzMjY5NzYx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img2](https://img-blog.csdn.net/201808171539347?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIzMjY5NzYx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img3](https://img-blog.csdn.net/20180817153942296?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIzMjY5NzYx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
